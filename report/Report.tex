\documentclass[11pt,a4paper]{article}
\usepackage{rotating} % <-- HERE
\usepackage{afterpage,lscape}
\usepackage{fullpage}
\usepackage{hyperref}
\usepackage{graphicx}
\graphicspath{ {images/} } 
\usepackage{mwe}% or load ’graphicx’ and ’blindtext’ manually
\usepackage{setspace}

\usepackage{commath}
\usepackage{mathtools}
\usepackage{amsmath}

\usepackage{fancyhdr}
\usepackage{subcaption} 
\pagestyle{fancy}
\fancyhf{}

\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}

\fancypagestyle{firstpagefooter} {
	\lfoot{\tiny{Version: 17.12.2019}}
	\cfoot{}
	\rfoot{\thepage}
	
}

\lfoot{Francesco Saverio Varini\\ Jakob Beckmann}
\rfoot{\thepage}

\begin{document}

\title{Reliable and Interpretable Artificial Intelligence\\ Project Report\\
 \normalsize{Autumn Semester 2019}}
\author{Francesco Saverio Varini \& Jakob Beckmann}


\maketitle

\thispagestyle{firstpagefooter}


\section{Abstract}

The intention of this report is to discuss about how it has been tackled the trade off between a fast but imprecise Interval Propagation analysis versus the slow but precise Linear Programming analysis on the output verification a network.\\Clearly, given the constraints for the project, namely the possibility to use only Gurobi linear programming and ELINA to  get better approximations of the nework output verification, some strategy and heuristics had to be taken into account.

\subsection{Full Layerwise Linear Programming}

For the smallest network, strictly speaking, the mnist\_relu\_3\_10, mnist\_relu\_3\_20, mnist\_relu\_3\_50, mnist\_relu\_6\_20, mnist\_relu\_6\_50 and mnist\_relu\_6\_100, no particular strategy has been taken into account, since all the networks above run with layerwise linear programming with all the possible epsilon between 0 and 0.1 in less than the timeout of 7 minutes.
Therefore, it was possible to obtain the best approximations in terms of verification using only Gurobi Linear programming.


\subsection{Heuristics}

For the bigger networks, namely, mnist\_relu\_4\_1024, mnist\_relu\_6\_200, mnist\_relu\_9\_100, mnist\_relu\_9\_200 different heuristics were deployed in order to speed up the verification using less LP variables and constraints, still improving as much as possible the precision.

\subsubsection{Neuronwise Heuristic}


Produce a scoring mechanism that scores a neuron based on some importance criteria and thus chooses the best neurons on which to perform linear programming per layer.

\subsubsection{Weight Scores Heuristic}

This heuristic looks at a neuron's outgoing weights and the neurons's bounds to determine its score. The score is obtained by multiplying the neuron's outgoing weights times either its  upper bound or the lower bound or the absolute difference between these. The scoring policy is quite slow in the current implementation.

\subsubsection{Moving Window Linear Programming Heuristic}

This heuristic, as the name suggests, consists on performing linear programming on an arbitrary window of layers of the network. This partial model is then moved across the network as if it were a window.
This heuristic verifies with very high precision but takes fractions of the time of the full layerwise linear programming.


\subsubsection{Recursive Back-Prop of High impact neurons}

This heuristic takes inspiration from the standard backpropagation procedure while training the neural networks.
Firstly, it is performed the fast interval propagation to have a general idea of the intervals each neuron inside the network can take. Then, starting at the output layer:
\begin{description}
  \item[$\bullet$ 1]  For each neuron $n$ in the layer $l$, check which neurons in the previous layer $l-1$ can affect its value the most. This is performed by checking the possible interval size of each incoming neuron $m$ and multiplying it by the weight between $m$ and $n$. This gives a general extimation how much $m$ affect the output interval of $n$. The scoring policy can be one of the one explained in the Neuronwise heuristic.
  \item[$\bullet$ 2]  Based on the scores computed in the previous step, take the highest capacity neurons that affect neuron $n$ and store them.
  \item[$\bullet$ 3]  Compute the union of all high impact sets returned.
  \item[$\bullet$ 4]  Repeat from step 1 using the previous layer (layer that so far contained the $m$ neurons), but only check for high impact neurons in the list returned from step 2.  
\end{description}
Back-propagate as explained above until the input layer is reached.\\ The algorithm end up having a list of high impact neuron sets for each layer.
Therefore, starting at the first hidden layer, linear programming is computed to better approximate the bounds of the high impact neurons. For the non-high impact neurons within the current layer,  interval propagation is performed in order to approximate their bounds.\\By default, all neurons in the output layer are considered "high impact", hence linear programming will always be performed on all output neurons.

\subsubsection{Final Remarks}



\end{document}
